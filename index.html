<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="SMILE is a self-supervised video learning method that improves semantic understanding and motion modeling by using CLIP-guided spatial features and synthetic motion patterns.">
  <meta name="keywords" content="Video Self-supervised Learning, Masked Video Modeling">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SMILE</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/kaust.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://fmthoker.github.io/">Fida Mohammad Thoker</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://orcid.org/0009-0006-4372-406X">Letian Jiang</a><sup>1</sup>,</span>  
            <span class="author-block">
              <a href="https://zhao-chen.com/">Chen Zhao</a><sup>1</sup></span>
            <span class="author-block">
              <a href="https://cemse.kaust.edu.sa/profiles/bernard-ghanem">Bernard Ghanem</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>King Abdullah University of Science and Technology (KAUST)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2504.00527"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2504.00527"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Github Link. -->
              <span class="link-block">
                <a href="https://github.com/fmthoker/SMILE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Model Link. -->
              <span class="link-block">
                  <a href="https://huggingface.co/fmthoker/SMILE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                        <img src="https://huggingface.co/front/assets/huggingface_logo.svg"
                            alt="Hugging Face" style="height:17px;">
                    </span>
                    <span>Models</span>
                  </a>
                  </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./assets/media/radar.png" alt="teaser" style="width:50%; padding-top:1%; display:block; margin:auto;" />

      <h2 class="subtitle has-text-justified">
      We introduce <b>SMILE</b>, a self-supervised video representation learning method that focuses on semantic abstraction and motion dynamics. Existing masked video modeling approaches, such as VideoMAE, primarily reconstruct low-level pixels from natural videos, which often contain significant temporal redundancy and limited motion variation.
      <b>SMILE</b> addresses these limitations by combining spatial semantics from pretrained image-language models like CLIP and synthetic motion augmentation to inject dynamic content into the training process. Instead of relying on raw pixel reconstruction, we guide the model to learn high-level visual concepts and motion-aware features.
      Notably, <b>SMILE</b> enables effective video representation learning even without natural videos, thanks to the use of synthetic motion overlays and trajectory-based masking. Through experiments across 7 datasets and diverse downstream tasks, <b>SMILE</b> consistently outperforms state-of-the-art self-supervised methods, demonstrating superior generalization and motion sensitivity.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <p>
            Our method introduces high-level <strong>spatial</strong> and <strong>motion semantics</strong> into masked video modeling to enhance video representation learning. We achieve this through two core innovations:
          </p>
          <ul>
            <li>
              <strong>Motion Augmentation:</strong>
              We overlay moving synthetic objects onto original videos along randomly generated smooth trajectories. These motions introduce dynamic visual patterns that reduce temporal redundancy and encourage the model to capture meaningful motion cues.
            </li>
            <li>
              <strong>Semantic Reconstruction with CLIP:</strong>
              Instead of reconstructing low-level pixels, we guide the model to reconstruct <em>CLIP features</em>, which encode high-level semantic information such as scenes and object interactions. This leads to more transferable and meaningful video representations.
            </li>
          </ul>
          <p>
            During training, we apply a hybrid masking strategy:
            <ul>
              <li><strong>Tube masking</strong> for original video regions (as in VideoMAE).</li>
              <li><strong>Trajectory-aware masking</strong> for synthetic objects, forcing the model to infer motion dynamics.</li>
            </ul>
          </p>
          <p>
            Our architecture follows a student-teacher design: the student is a transformer-based encoder-decoder trained to reconstruct masked features, while the teacher is a frozen CLIP encoder providing high-level supervision.
          </p>
          <p>
            By combining synthetic motion, semantic supervision, and targeted masking, our method learns temporally-aware and semantically-rich video representations. Notably, it achieves strong downstream performance even without access to natural video data.
          </p>
          <img src="./assets/media/main.png " alt="method" style="width:100%; padding-top:1%;"/>  
          <img src="./assets/media/motion.png" alt="motion" style="width:100%; display:inline-block; padding-top:2%;" />
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Highlights</h2>
        <div class="content has-text-justified">
          <p>
            Our method achieves state-of-the-art performance across multiple video understanding benchmarks, demonstrating strong generalization in both semantic and motion-centric tasks.
          </p>

        <div>
          <h3 class="title is-4">I. State-of-the-art on SSv2 and K400 </h3>
          <div class="observations domain">
            <p>
            We achieve top results on both <strong>Something-Something V2</strong> (SSv2) and <strong>Kinetics-400</strong> (K400) benchmarks using full finetuning with a ViT-B backbone. Our method outperforms prior self-supervised video models, including MGMAE, MME, and MGM, with significant gains—up to <strong>2.5%</strong> on SSv2 and <strong>2.3%</strong> on K400.
            </p>
            <p>
            Even with fewer training epochs, our model surpasses existing methods trained for longer, thanks to <em>CLIP-based semantic supervision</em> that offers a more efficient and meaningful learning signal than traditional pixel-level reconstruction.
            </p>
          <div style="font-size:0;">
            <img src="./assets/media/K400.png " alt="ablation" style="width:50%; "/>
            <img src="./assets/media/SSv2.png " alt="ablation" style="width:50%; "/>
          </div>
          </div>
        </div>

        <div>
          <h3 class="title is-4">II. Downstream Generalization</h3>
          <div class="observations samples">
            <p>
            We evaluate our method on the <strong><a href="https://bpiyush.github.io/SEVERE-website/"> SEVERE benchmark </a></strong>, which tests four key generalization factors: <em>domain shift</em>, <em>sample efficiency</em>, <em>action granularity</em>, and <em>task shift</em>. Our model consistently outperforms existing approaches on all aspects.
            </p>
            <ul>
              <li><strong>Domain Shift:</strong> Best performance on SSv2 and Gym99 despite pretraining on K400.</li>
              <li><strong>Low-shot Learning:</strong> 5% higher accuracy on GYM with just 1,000 samples, outperforming MME.</li>
              <li><strong>Fine-grained Actions:</strong> Top results on FX-S1 and UB-S1, surpassing motion-focused models like MGMAE and MGM.</li>
              <li><strong>Task Adaptability:</strong> 9% improvement on multi-label Charades recognition and strong results on repetition counting.</li>
            </ul>
            <p>
            Overall, our full model with synthetic motion achieves a <strong>3.0% average gain</strong> over strong baselines, demonstrating robust generalization across diverse video understanding scenarios.
            </p>
          <img src="./assets/media/severe.png " alt="grad-cam" style="width:100%; "/>
          </div>
        </div>

        <div>
          <h3 class="title is-4">III. Better CLIP Adaptation</h3>
          <div class="observations actions">
            <p>
            Compared to CLIP-based methods like <strong>ViCLIP</strong> and <strong>UMT</strong>, our model achieves higher accuracy on motion-sensitive datasets, especially under <em>linear probing</em>. This suggests stronger video representations, learned with less data and without relying on video-text alignment.
            </p>
          <img src="./assets/media/CLIP.png " alt="data-eff" style="width:100%; "/>
          </div>
        </div>

        <div>
          <h3 class="title is-4">IV. Learning Without Natural Videos</h3>
          <div class="observations tasks">
            <p>
            We show that our method can learn strong video representations even without any natural video data. By overlaying synthetic object motions onto static backgrounds—including <em>black</em> and <em>noise images</em>—our model achieves <strong>22.2%</strong> on K400<sub>m</sub> and <strong>23.0%</strong> on SSv2<sub>m</sub>, significantly outperforming random initialization.
            </p>
            <p>
            This highlights a new learning paradigm: masked video modeling driven purely by motion, without relying on real-world videos.
            </p>
            <div style="text-align: center;">
              <img src="./assets/media/unnatural_example.png " alt="severe" style="width:70%; "/>
              <img src="./assets/media/unnatural.png " alt="severe" style="width:70%; "/>
            </div> 
          </div>
        </div>

        <div>
          <h3 class="title is-4">V. Qualitative Analysis</h3>
          <div class="observations tasks">
            <p>
            To assess temporal dynamics in learned video representations, we visualize feature similarities across frames (see Figure&nbsp;4). Compared to other self-supervised methods, our model exhibits greater feature variation over time—indicating stronger temporal awareness. 
            </p>
            <div style="text-align: center;">
              <img src="./assets/media/sim_main.png" alt="sim_main" style="width:70%; display:inline-block; padding-top:2%;" />
            </div> 
          </div>
        </div>

        </div>
      </div>
    </div>
    <!--/ Highlights. -->


  </div>
</section>

<style>
  .observations {
    padding-top: 2%;
    padding-bottom: 2%;
    padding-left: 5%;
    padding-right: 5%;
    margin-bottom: 3%;
  }
  .domain {
    background: #FFFFFF;
  }
  .samples {
    background: #FFFFFF;
  }
  .actions {
    background: #FFFFFF;
  }
  .tasks {
    background: #FFFFFF;
  }
</style>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{thoker2025smile,
  author    = {Thoker, Fida Mohammad and Jiang, Letian and Zhao, Chen and Ghanem, Bernard},
  title     = {SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning},
  journal   = {CVPR},
  year      = {2025},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2504.00527">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/fmthoker/SMILE" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website is based on the template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
